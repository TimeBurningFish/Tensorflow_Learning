{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mnist手写数字识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 28\n",
    "chunk_n = 28\n",
    "\n",
    "rnn_size = 256\n",
    "\n",
    "n_output_layer = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### placeholder（占位符）(dtype=tf.float32, shape=[144, 10], name='X')\n",
    "dtype：数据类型，必填，默认为value的数据类型，传入参数为tensorflow下的枚举值（float32，float64.......）\n",
    "\n",
    "shape：数据形状，选填，不填则随传入数据的形状自行变动，可以在多次调用中传入不同形状的数据\n",
    "\n",
    "name：常量名，选填，默认值不重复，根据创建顺序为（Placeholder，Placeholder_1，Placeholder_2.......）\n",
    "\n",
    "占位符是tf流程图的起始节点，另外，RNN状态向量也是存储在占位符中，在每一次运行后更新输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder('float', [None, chunk_n, chunk_size])\n",
    "Y = tf.placeholder('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable 通过Variable（注：V大写）方法创建，并且需要传递初始值。在使用前需要通过tensorflow的初始化方法进行初始化。\n",
    "W = tf.Variable(initial_value=tf.zeros([9, 5]),  # 初始值，必填，张量或可以转换为张量的Python对象。初始值必须有指定一个形状，除非`validate_shape`设置为False。（形状9,5）\n",
    "                \n",
    "                trainable=True,  # 如果`True`，则默认值也将变量添加到图形中集合`GraphKeys.TRAINABLE_VARIABLES`。这个集合用作“Optimizer”类使用的默认变量列表\n",
    "                \n",
    "                collections=None,  # 图表集合键的列表。新的变量被添加到这些集合。默认为`[GraphKeys.GLOBAL_VARIABLES]`。\n",
    "                \n",
    "                validate_shape=True,  # 如果`False`，允许变量用初始化未知形状的值。如果“True”，默认的形状`initial_value`必须是已知的。\n",
    "                \n",
    "                caching_device=None,  # 可选设备字符串，描述变量的位置应该被缓存以供阅读。默认为变量的设备。如果不是“None”，则缓存在另一个设备上。典型的用途是缓存在使用变量的Ops所在的设备上进行重复数据删除复制`Switch`和其他条件语句。\n",
    "                \n",
    "                name='W',  # 变量的可选名称。默认为“Variable”并获取自动去重（Variable_1,Variable_2....）。\n",
    "                \n",
    "                variable_def=None, # `VariableDef`协议缓冲区。如果不是“无”，则重新创建变量对象及其内容，引用变量的节点在图中，必须已经存在。图形没有改变。`variable_def`和其他参数是互斥的。\n",
    "                \n",
    "                dtype=tf.float32, # 如果设置，initial_value将被转换为给定的类型。如果`None'，数据类型将被保存（如果`initial_value`是一个张量），或者“convert_to_tensor”来决定。\n",
    "                \n",
    "                expected_shape=None,  # 张量的Shape。如果设置，initial_value需要符合这个形状。\n",
    "                \n",
    "                import_scope=None)  # 可选的字符串。名称范围添加到`Variable.`仅在从协议缓冲区初始化时使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### transpose(a, perm=None, name=\"transpose\"):\n",
    "\n",
    "perm:控制转置的操作,以perm = [0,1,2] 3个维度的数组为例, 0--代表的是最外层的一维, 1--代表外向内数第二维, 2--代表最内层的一维,这种perm是默认的值\n",
    "\n",
    "#### tf.reshape(data, [-1, chunk_size])\n",
    "-1代表自动计算\n",
    "\n",
    "#### tf.split(dimension, num_split, input)：\n",
    "dimension的意思就是输入张量的哪一个维度，如果是0就表示对第0维度进行切割。num_split就是切割的数量，如果是2就表示输入张量被切成2份，每一份是一个列表。\n",
    "\n",
    "#### 单层rnn：tf.contrib.rnn.static_rnn：\n",
    "\n",
    "输入：[步长,batch,input] \n",
    "输出：[n_steps,batch,n_hidden] \n",
    "\n",
    "#### 多层rnn：tf.nn.dynamic_rnn：\n",
    "\n",
    "输入：[batch,步长,input] \n",
    "输出：[batch,n_steps,n_hidden] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recurrent_neural_network(data):##data:训练集 无标签样本\n",
    "      layer = {'w_':tf.Variable(tf.random_normal([rnn_size, n_output_layer])), 'b_':tf.Variable(tf.random_normal([n_output_layer]))}\n",
    "  \n",
    "      lstm_cell = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "      print(data.shape)\n",
    "      \n",
    "      data = tf.transpose(data, [1,0,2])#为数组的转置函数，转置最外两层\n",
    "      \n",
    "      print(data.shape)\n",
    "      \n",
    "      data = tf.reshape(data, [-1, chunk_size])##-1代表自动计算\n",
    "    \n",
    "      data = tf.split(data, chunk_n)##向量分割\n",
    "    \n",
    "      outputs, status = tf.contrib.rnn.static_rnn(lstm_cell, data, dtype=tf.float32)\n",
    "  \n",
    "      ouput = tf.add(tf.matmul(outputs[-1], layer['w_']), layer['b_'])\n",
    "    #output=outputs[-1] 就取到了最后一步的ouput\n",
    "    #matmul 矩阵相乘\n",
    "    #add矩阵相加\n",
    "      return ouput "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.reduce_mean(input_tensor, axis=None, keep_dims=False, name=None, reduction_indices=None)\n",
    "可跨越维度的计算张量各元素的平均值\n",
    "#### softmax_cross_entropy_with_logits(_sentinel=None, # pylint: disable=invalid-name,labels=None,logits=None,dim=-1, name=None)      \n",
    "_sentinel：该参数为内部使用，实际运用中不会使用。目的是占据函数参数的第一个位置，让我们不能通过位置传递labels 和 logits，而是使用关键字传递参数，避免混淆\n",
    "labels: 实际标签\n",
    "logits: 对数几率，需要注意的是这个函数内部自动计算 softmax，然后再计算交叉熵代价函数，也就是说 logits 必须是没有经过 tf.nn.softmax 函数处理的数据，否则导致训练结果有问题。\n",
    "#### AdamOptimizer\n",
    "___init__(learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, use_locking=False, name='Adam')\n",
    "                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(X, Y):\n",
    "    predict = recurrent_neural_network(X)\n",
    "    cost_func = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=predict))##preddict 一定没有经过softmax\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost_func)\n",
    "    epochs = 10\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        for epoch in range(epochs):\n",
    "                epoch_loss = 0\n",
    "                for i in range(int(mnist.train.num_examples/batch_size)):\n",
    "                    x,y = mnist.train.next_batch(batch_size)\n",
    "                    x = x.reshape([batch_size,chunk_n,chunk_size])\n",
    "                    _,c = sess.run([optimizer,cost_func],feed_dict = {X:x,Y:y})\n",
    "                    epoch_loss += c\n",
    "                print(epoch,':',epoch_loss)\n",
    "#                 acc = tf.reduce_mean(tf.cast(correct,'float'))\n",
    "#                 print('accuracy: ',acc.eval({X:mnist.test.images.reshape(-1,chuk_n,chunk_size),Y:mnist.test.lables}))\n",
    "        print('pre',predict.shape,'\\n',predict[-1],'\\n','Y',Y.shape)\n",
    "        \n",
    "        correct = tf.equal(tf.argmax(predict,1), tf.argmax(Y,1))\n",
    "        acc = tf.reduce_mean(tf.cast(correct,'float'))\n",
    "        print('accuracy: ',acc.eval({X:mnist.test.images.reshape(-1,chunk_n,chunk_size),Y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 28, 28)\n",
      "(28, ?, 28)\n",
      "WARNING:tensorflow:From /home/blueberry/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "0 : 210.895644974\n",
      "1 : 66.2925029853\n",
      "2 : 43.7067454709\n",
      "3 : 34.1231181358\n",
      "4 : 27.134102992\n",
      "5 : 22.7766535276\n",
      "6 : 19.1672370344\n",
      "7 : 17.2465875364\n",
      "8 : 14.9541022436\n",
      "9 : 12.6869191339\n",
      "pre (?, 10) \n",
      " Tensor(\"strided_slice:0\", shape=(10,), dtype=float32) \n",
      " Y <unknown>\n",
      "accuracy:  0.9852\n"
     ]
    }
   ],
   "source": [
    "train_neural_network(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scope\n",
    "#### Name_scope:\n",
    "with tf.name_scope(name):\n",
    "在这里创建的Variable通过tf.Varialble 创建的Variable都会在Variable 前加入 name/variable_name\n",
    "表示在name底下创建了variable\n",
    "并且如果创建时只定variavble_name在同namescope下会使viriable自动修改名字\n",
    "\n",
    "#### Variable_scope:\n",
    "通过tf.get_variable创建也会有名字\n",
    "并且想要重复使用某一个Variable可以通过tf.get_variable（name）只输入名字就可以调用前面的变量了\n",
    "并且在这个重复利用的语句前面需要嗲用scope（with .... as scope）.resuse_variable()\n",
    "\n",
    "Variable 创建方式：tf.get_varialble:需要指定initializer tf.Varialble\n",
    "\n",
    "为什么需要使用Scope：RNN需要循环利用\n",
    "例：training 和 Testing中的网络结构不同但是，test中需要使用train的Variable\n",
    "\n",
    "首先在同一个scope下创建好trainRNN，然后使用testRNN创建，创建过程中所有的Variable参数在"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
